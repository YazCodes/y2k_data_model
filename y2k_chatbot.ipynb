{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQPZe96DhBf2Za4Jlhkpsr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YazCodes/y2k_data_model/blob/main/y2k_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HviTlysh0Z9L"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "id": "UxX0LWs_3VvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets peft accelerate bitsandbytes\n"
      ],
      "metadata": {
        "id": "vFKmm6UU3YxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('text', data_files={'train': 'chat_data.txt'})\n",
        "dataset"
      ],
      "metadata": {
        "id": "_WxcSrIR5fUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer # turns words into tokens\n",
        "\n",
        "model_name = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token # ensures all inputs have the same lenght\n",
        "\n",
        "def tokenize_function(examples): # a function converting raw text (the training data) into tokens (for the model to understand)\n",
        "  tokenized_inputs = tokenizer(\n",
        "      examples[\"text\"],\n",
        "      truncation =True,\n",
        "      padding = \"max_length\",\n",
        "      max_length = 128\n",
        "  )\n",
        "  tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy() # Add labels for training\n",
        "  return tokenized_inputs\n",
        "\n",
        "# applying the tokenize_function to every row of my dataset\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "rv8avUuV56_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],  # GPT2 attention layers\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n"
      ],
      "metadata": {
        "id": "vnMexPwI6IgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./y2k_model\",\n",
        "    # evaluation_strategy=\"no\", # Removed unsupported argument\n",
        "    learning_rate=5e-4,\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"]\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "h3nnugEUDDLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"y2k_model\")\n",
        "tokenizer.save_pretrained(\"y2k_model\")\n"
      ],
      "metadata": {
        "id": "pgpwtOfjDHRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the model"
      ],
      "metadata": {
        "id": "KU7Kkv6LDIAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"y2k_model\", tokenizer=tokenizer)\n",
        "\n",
        "prompt = \"User: What accessories for a Y2K outfit?\\nBot:\"\n",
        "print(generator(prompt, max_length=80, do_sample=True, temperature=0.8)[0][\"generated_text\"])\n"
      ],
      "metadata": {
        "id": "k7Ydz_t6DOkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n"
      ],
      "metadata": {
        "id": "smkJbcWXEHW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__file__)\n"
      ],
      "metadata": {
        "id": "PhZr12jJErPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cba56d6e"
      },
      "source": [
        "!pip install trl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92219e21"
      },
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    args=training_args,\n",
        "    # max_seq_length=128, # Removed unsupported argument\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}